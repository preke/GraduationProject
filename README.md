# Graduation-Project
# 毕设流水账

---

# Part 1 :
**文本分析，提取关键词，判断文本反映的信息是利好还是利空**

##思路：
### \#1：
先构建一个dict, key是一些关于股票的关键词，然后给每个关键词赋一个权重，标识利好的词权值为正，标识利空的权值为负，然后拿这些关键词来扫文章，计算这些所有权重的总和，来判断利好还是利空。
然后再拿股票在这个时间段的真实变动来修正初始值（梯度下降的思想），做出一个模型。

难点：

 * dict的构建，怎样找到一个金融的字典（尝试边爬边构建？）
 * 实时热点的作用的弱化
### \#2：
提取关键字，然后给关键字赋权。但是怎样判断关键词的利好还是利空呢？

## 2016/12/10
python关键词提取，搜索结合以前的见闻有以下几个知道的词：

* TextRank
* jieba
* TF-IDF

## 2016/12/12
了解了TD-IDF的基本原理，那么看一下手头的工具：

* jieba分词应该可以用来分词和统计词频
* TF-IDF用来提取关键字，或者生成摘要。但是需要一个语料库

关键是这个语料库怎么找，金融的语料库还是更一般的语料库。
我分析的应该都是金融类的文章，所以如果用更一般的语料库，会不会造成很多不必要的工作量

## 2016/12/15
先决定是提取文本的关键字
查到了jieba分词自带的tf-idf和idf.txt。
觉得应该再在分析文章的时候应该将所有的句子整合到一个str里面会好一点

想到一个问题，能够查到的公告和新闻大多都是利好的，这点怎么破。

现在可以通过```jieba.analyse.extract_tags()```提取出文档的关键字
但是关键字怎么处理；关键字的数字又怎么比较呢？

组会听了万山的分享和阳哥的建议，提取出的关键词再去各大新闻网站去搜用户评论；
关键是，这个用户评论是没有权威性的，我们做出来之后只能说是用户情感分析


## 2017/2/13
复习了一下svm，准备着手

情感分析：
数据源: 各大财经类网站的新闻 和 评论
then 分词
then 分类
短期先做到这点

问了一下万山，具体金融文本分析的规程和svm该怎样使用，暂时还没有答复

明天确定几个网站，着手开始写爬虫

## 2017/2/14
万山给了几个好用的词典，可以在jieba分词中用自定义的词典
存在这样一个问题，不能直接从财经网站上爬，而是百度股市通
爬取的200篇永清环保的新闻

## 2017/2/26
全面理解了朴素贝叶斯，觉得用这样的二分类模型会更好一点，也更易于实现。
然后基于朴素贝叶斯实现了一套文本分类的方法，不过分词导致词集有9000+个词语，这还仅仅是一支股票+去停用词

## 2017/2/27
跟阳哥讨论了一下，主要精力还是放在第一部分的，只要把第一部分的情感分析做出来，剩下的都好说
阳哥给的建议是，Naive Bayes和word2vec结合一下，或者是比较一下哪个效果更好。

## 2017/3/2
看了一下后续的Ng的公开课，恰巧讲到我遇到的模型，数据维度太大，可以重新建立一个不单纯用0,1来表示的一种文本向量，接下来尝试

## 2017/3/21
感谢这个比赛，nlp还是有很多个点可以想的

现在遇到的问题就是，遇到一些，很多，中性的文本不知道该怎么处理
正好最近的比赛也用到w2v, 我觉得可以借鉴这边的思想
先用确定的正负文本训练一下模型，然后中性的文本跟他们做相似性检测，然后定向归类
注意的点：
- 无论文本的长短，都要映射到相同的维度中去考虑
- tf-idf看要不要考虑
- 如果采用w2v的话，会不会样本太少
在此也引发了一个问题，就是：**用w2v来训练模型，和用NB， 哪个效果更好，该怎样去评判呢 **
